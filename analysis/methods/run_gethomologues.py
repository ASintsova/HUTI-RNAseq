import subprocess
import shlex
import glob
import shutil
import pandas as pd
import os
import re


import helpers


def run_get_homologs(gh_bin, gbk_dir, I, C, S, t, exclude_paralogs, run_id, clean_up=False, core=True):

    # Store current working directory
    cwd = os.getcwd()
    # Make a temp directory and go there
    if not os.path.exists(run_id):
        os.makedirs(run_id)
    os.chdir(run_id)

    # Copy gbk files specified in I into a new genome directory
    os.makedirs('genomes')
    with open(I, "r") as gf:
        for line in gf:
            print(line)
            file = os.path.join(gbk_dir, line.strip())
            shutil.copy(file, 'genomes')
    num_genomes = len(os.listdir("genomes"))
    # Run gh with appropriate params
    # Now there's not I, and gbk dir is hard coded
    e = " -e" if exclude_paralogs else ''
    e_name = "1" if exclude_paralogs else "0"
    # Running COGtriangle algorithm
    gh_script = os.path.join(gh_bin, "get_homologues.pl")
    # By default will only include clusters with at least one seq per genome (BDBH by default)

    cmd0_str = "{} -d genomes -C {} -S {} -c{}".format(gh_script, C, S, e)
    print(cmd0_str)
    cmd0 = shlex.split(cmd0_str)
    subprocess.call(cmd0)
    # Running COG algorithm
    cmd1_str = "{} -d genomes -G -t {} -C {} -S {} -A -c{}".format(gh_script, t, C, S, e)
    print(cmd1_str)
    cmd1 = shlex.split(cmd1_str)
    subprocess.call(cmd1)
    # Running OMCL algorithm
    cmd2_str = "{} -d genomes -M -t {} -C {} -S {} -A -c{}".format(gh_script, t, C, S, e)
    print(cmd2_str)
    cmd2 = shlex.split(cmd2_str)
    subprocess.call(cmd2)
    gh_out_dir = "genomes_homologues"
    cog_out = glob.glob(gh_out_dir+"/*algCOG_e{}_C{}_S{}_".format(e_name, str(C), str(S)))[0]
    omcl_out = glob.glob(gh_out_dir+"/*algOMCL_e{}_C{}_S{}_".format(e_name, str(C), str(S)))[0]
    bdbh_out = glob.glob(gh_out_dir+"/*algBDBH_e{}_C{}_S{}_".format(e_name, str(C), str(S)))[0]

    print(cog_out)
    print(omcl_out)
    print(bdbh_out)
    if cog_out.split("alg")[0] == omcl_out.split("alg")[0]:
        print("Go!")
        if core:
            cmd3_str = "{} -o {} " \
                       "-d {},{},{},  -t {} -m".format(os.path.join(gh_bin, "compare_clusters.pl"),
                                                       os.path.join(run_id, "core_genome"),
                                                       cog_out, omcl_out, bdbh_out, num_genomes)

            print(cmd3_str)

        else:
            cmd3_str = "{} -o {} " \
                       "-d {},{}, -t 0 -m".format(os.path.join(gh_bin, "compare_clusters.pl"),
                                                  os.path.join(run_id, "pangenome"),
                                                  cog_out, omcl_out)
        cmd3 = shlex.split(cmd3_str)
        subprocess.call(cmd3)
    # Delete everything except for .tab and _list files
    if clean_up:
        for root, dirs, files in os.walk("genomes_homologues", topdown=False):
            for name in files:
                if '.tab' in name or '_list' in name:
                    print(os.path.join(root, name))
                else:
                    os.remove(os.path.join(root, name))
            for name in dirs:
                fold = os.path.join(root, name)
                print(fold)
                for f in os.listdir(fold):
                    shutil.move(os.path.join(fold, f), root)
            for name in dirs:
                if not os.listdir(os.path.join(root, name)):
                    os.removedirs(os.path.join(root, name))

    # Change back to original working directory
    os.chdir(cwd)


def cross_ref_from_core_genome(ogc_dir, output_file):
    """
    This function takes in the directory of core OGC generated by get_homologues/compare_clusters
    and returns a table of all orthologous genes (PROKKAs adn bnums for MG1655).

    Note to self: for MG1655 gbk file protein id were deleted so that get_homologues returns
    locus tags instead

    Generates csv file in the same directory as pangenome matrix ending with 'cross_ref.csv'
    """
    strain_cleanup = {"HM1": "HM01", "HM3": "HM03", "HM6": "HM06", "HM7": "HM07"}
    faa_files = [os.path.join(ogc_dir, f) for f in os.listdir(ogc_dir) if ".faa" in f]
    print("Number of core genes: {}".format(len(faa_files)))
    core_genome = {}
    for faa in faa_files:
        clinical_genes = []
        with open(faa, "r") as fh:
            for line in fh:
                if line.startswith(">"):
                    words = line.split("|")
                    gene_id = words[0].split("ID:")[1].strip()
                    strain = re.split("[_():;]", words[5])[0].strip()
                    if strain == "U00096":
                        key = gene_id
                    else:
                        if strain in strain_cleanup.keys():
                            strain = strain_cleanup[strain]
                        clinical_genes.append((strain, gene_id))
            try:
                assert(len(clinical_genes)) == 14
                core_genome[key] = sorted(clinical_genes)
            except UnboundLocalError:
                print("Missing key")
                continue
    with open(output_file, "w") as fo:
        title = "HM01,HM03,HM06,HM07,HM14,HM17,HM43,HM54,HM56,HM57,HM60,HM66,HM68,HM86"
        fo.write(",{}\n".format(title))
        for key, val in core_genome.items():
            genomes = ",".join([v[0] for v in val])
            assert genomes == title
            genes = [key] + [v[1] for v in val]
            fo.write(",".join(genes) + "\n")
    return core_genome


def core_counts_table(cross_ref, count_dir, tpm_dir, output_file):
    conditions = ["UR", "UTI"]

    df = pd.read_csv(cross_ref, index_col=0)

    for genome in df.columns:
        print(genome)
        for cond in conditions:
            print(cond)
            count_file = os.path.join(count_dir, "{}_{}_trimmed_sorted_counts".format(genome, cond))
            tpm_file = os.path.join(tpm_dir, "{}_{}_trimmed_sorted_counts_tpm.csv".format(genome, cond))
            count_dict = helpers.process_count_file(count_file)
            tpm_dict = helpers.process_count_file(tpm_file, ",")
            counts = []
            tpms = []
            for prokka in df[genome].values:
                counts.append(count_dict.get(prokka, "NaN"))
                tpms.append(tpm_dict.get(prokka, "NaN"))
            df["{}_{}_count".format(genome, cond)] = counts
            df["{}_{}_tpm".format(genome, cond)] = tpms
    df.to_csv(output_file)
    return df


###################################################################################################


def run_final_settings():
    config_file = os.path.join((os.path.dirname(os.path.realpath(__file__))), "config")
    config_dict = helpers.process_config(config_file)
    gh_bin = config_dict["get_homologs"]["path"]
    gbks = config_dict["get_homologs"]["gbk_dir"]
    gbk_list_file = config_dict["get_homologs"]["gbk_list"]
    print(gbk_list_file)
    out_dir = config_dict["out_dir"]["path"]
    out_dir = os.path.join(out_dir, "C50_S90_e1_")
    run_get_homologs(gh_bin, gbks, gbk_list_file, 50, 90, 0, True, out_dir)


def make_final_ortholog_table():

    ogc = "/Users/annasintsova/git_repos/HUTI-RNAseq/results/methods/C50_S90_e1_/core_genome"
    out_file = "/Users/annasintsova/git_repos/HUTI-RNAseq/results/methods/orthologs.csv"
    cross_ref_from_core_genome(ogc, out_file)


def get_counts_for_orthologs():
    config_file = os.path.join((os.path.dirname(os.path.realpath(__file__))), "config")
    config_dict = helpers.process_config(config_file)
    cross_ref = config_dict["get_homologs"]["cross_ref"]
    count_dir = config_dict["out_dir"]["counts"]
    tpm_dir = config_dict["out_dir"]["tpms"]
    out_dir = config_dict["out_dir"]["path"]
    output_file = os.path.join(out_dir, "core_counts_and_tpms.csv")
    df = core_counts_table(cross_ref, count_dir, tpm_dir, output_file)
    print(df.head())


if __name__ == "__main__":

    # run_final_settings()
    # make_final_ortholog_table()
    get_counts_for_orthologs()
